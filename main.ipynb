{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0746e4a8-4a2e-43d4-a696-6ffbaa61dd10",
   "metadata": {},
   "source": [
    "# Cond√© Nast Formula 1 Data Engineer Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cac7f09-da78-4dd4-9d2c-499ac9745552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ab7d285-1f34-4ec5-a838-6916803af381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path that holds the data\n",
    "dataDirectory = \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27010396-f19c-4b36-9e41-97652ec740c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store filenames in list\n",
    "files = prep.getFileNames(dataDirectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcddadef-02a9-4d9d-bef4-7f559b84f179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cc_out.csv', 'circuits.csv', 'constructors.csv', 'constructor_results.csv', 'constructor_standings.csv', 'drivers.csv', 'driver_standings.csv', 'lap_times.csv', 'pit_stops.csv', 'qualifying.csv', 'races.csv', 'results.csv', 'seasons.csv', 'status.csv']\n"
     ]
    }
   ],
   "source": [
    "# Display all name of all files within the data folder\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4678bab-f76c-49fa-b5dd-5d805bf023cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o32.load.\n: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:645)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1230)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1435)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:493)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1868)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1910)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678)\r\n\tat org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:225)\r\n\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$1(HadoopFSUtils.scala:95)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:85)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:69)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:158)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:131)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:94)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:66)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.createInMemoryFileIndex(DataSource.scala:581)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:417)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:325)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:307)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:307)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:239)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:567)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:830)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10900/3987339149.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Build dataframes for Formula 1 datasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mvars\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"_df\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprep\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataDirectory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Dataframe names are concentated using filename + _df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\GitHub_projects\\CondeNestDEChallenge\\prep.py\u001b[0m in \u001b[0;36mcreateDF\u001b[1;34m(directoryPath, file)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcreateDF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectoryPath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"header\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"true\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"inferSchema\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"true\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectoryPath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mavgTimeSpentAtPitStop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriverDf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpitStopDf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o32.load.\n: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:645)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1230)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1435)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:493)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1868)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1910)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:678)\r\n\tat org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:225)\r\n\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$1(HadoopFSUtils.scala:95)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:85)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:69)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:158)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:131)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:94)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:66)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.createInMemoryFileIndex(DataSource.scala:581)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:417)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:325)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:307)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:307)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:239)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:567)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:830)\r\n"
     ]
    }
   ],
   "source": [
    "# Build dataframes for Formula 1 datasets\n",
    "for file in files:\n",
    "    vars()[file.split('.')[0] + \"_df\"] = prep.createDF(dataDirectory, file) # Dataframe names are concentated using filename + _df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4753327b-03ef-4818-90d9-d1930d9f3f1e",
   "metadata": {},
   "source": [
    "## Basic exploration data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e63a4a-dc5a-4347-a902-a5fb5685e958",
   "metadata": {},
   "source": [
    "### Display first 5 rows of a few DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3d1204a-0c4d-48a8-b7b5-753af672f2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+--------------------+------------+---------+--------+-------+---+--------------------+\n",
      "|circuitId| circuitRef|                name|    location|  country|     lat|    lng|alt|                 url|\n",
      "+---------+-----------+--------------------+------------+---------+--------+-------+---+--------------------+\n",
      "|        1|albert_park|Albert Park Grand...|   Melbourne|Australia|-37.8497|144.968| 10|http://en.wikiped...|\n",
      "|        2|     sepang|Sepang Internatio...|Kuala Lumpur| Malaysia| 2.76083|101.738| \\N|http://en.wikiped...|\n",
      "|        3|    bahrain|Bahrain Internati...|      Sakhir|  Bahrain| 26.0325|50.5106| \\N|http://en.wikiped...|\n",
      "|        4|  catalunya|Circuit de Barcel...|    Montmel√≥|    Spain|   41.57|2.26111| \\N|http://en.wikiped...|\n",
      "|        5|   istanbul|       Istanbul Park|    Istanbul|   Turkey| 40.9517| 29.405| \\N|http://en.wikiped...|\n",
      "+---------+-----------+--------------------+------------+---------+--------+-------+---+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Circuits data\n",
    "circuits_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ff57fd-28a3-4481-a8df-7103cc72bb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "prep.exportDF(circuits_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98216b85-210e-49a1-925e-a81b3ce45d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+--------+-------------+------+----+--------+------------+-------------+------+----+-----------+------------+----------+----+--------------+---------------+--------+\n",
      "|resultId|raceId|driverId|constructorId|number|grid|position|positionText|positionOrder|points|laps|       time|milliseconds|fastestLap|rank|fastestLapTime|fastestLapSpeed|statusId|\n",
      "+--------+------+--------+-------------+------+----+--------+------------+-------------+------+----+-----------+------------+----------+----+--------------+---------------+--------+\n",
      "|       1|    18|       1|            1|    22|   1|       1|           1|            1|  10.0|  58|1:34:50.616|     5690616|        39|   2|      1:27.452|        218.300|       1|\n",
      "|       2|    18|       2|            2|     3|   5|       2|           2|            2|   8.0|  58|     +5.478|     5696094|        41|   3|      1:27.739|        217.586|       1|\n",
      "|       3|    18|       3|            3|     7|   7|       3|           3|            3|   6.0|  58|     +8.163|     5698779|        41|   5|      1:28.090|        216.719|       1|\n",
      "|       4|    18|       4|            4|     5|  11|       4|           4|            4|   5.0|  58|    +17.181|     5707797|        58|   7|      1:28.603|        215.464|       1|\n",
      "|       5|    18|       5|            1|    23|   3|       5|           5|            5|   4.0|  58|    +18.014|     5708630|        43|   1|      1:27.418|        218.385|       1|\n",
      "+--------+------+--------+-------------+------+----+--------+------------+-------------+------+----+-----------+------------+----------+----+--------------+---------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Results data\n",
    "results_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a605994-0910-4353-861e-e82c26b1dbd4",
   "metadata": {},
   "source": [
    "## Challenge 1: Average time each driver spent at the pit stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6191bfac-79b5-4299-a03d-4fe880ffd270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----+---+--------+--------+------------+\n",
      "|raceId|driverId|stop|lap|    time|duration|milliseconds|\n",
      "+------+--------+----+---+--------+--------+------------+\n",
      "|   841|     153|   1|  1|17:05:23|  26.898|       26898|\n",
      "|   841|      30|   1|  1|17:05:52|  25.021|       25021|\n",
      "|   841|      17|   1| 11|17:20:48|  23.426|       23426|\n",
      "|   841|       4|   1| 12|17:22:34|  23.251|       23251|\n",
      "|   841|      13|   1| 13|17:24:10|  23.842|       23842|\n",
      "+------+--------+----+---+--------+--------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pit_stops_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d79acad-a797-4c74-9666-bb8e3691e6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+----+--------+----------+----------+-----------+--------------------+\n",
      "|driverId| driverRef|number|code|forename|   surname|       dob|nationality|                 url|\n",
      "+--------+----------+------+----+--------+----------+----------+-----------+--------------------+\n",
      "|       1|  hamilton|    44| HAM|   Lewis|  Hamilton|1985-01-07|    British|http://en.wikiped...|\n",
      "|       2|  heidfeld|    \\N| HEI|    Nick|  Heidfeld|1977-05-10|     German|http://en.wikiped...|\n",
      "|       3|   rosberg|     6| ROS|    Nico|   Rosberg|1985-06-27|     German|http://en.wikiped...|\n",
      "|       4|    alonso|    14| ALO|Fernando|    Alonso|1981-07-29|    Spanish|http://en.wikiped...|\n",
      "|       5|kovalainen|    \\N| KOV|  Heikki|Kovalainen|1981-10-19|    Finnish|http://en.wikiped...|\n",
      "+--------+----------+------+----+--------+----------+----------+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "drivers_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf733d7e-b73f-4942-a8fd-9effd37e3a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average is calculate by milliseconds\n",
    "avgPitstopTimeByDriver = prep.avgTimeSpentAtPitStop(drivers_df, pit_stops_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b98b5ff3-6f0d-4d81-88ff-34778db81728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+------------------+\n",
      "|forename|  surname| avg_pit_stop_time|\n",
      "+--------+---------+------------------+\n",
      "|  Daniil|    Kvyat| 76381.00930232558|\n",
      "|   Lance|   Stroll| 65902.95384615385|\n",
      "|   Kamui|Kobayashi|34831.495327102806|\n",
      "|    Jack|   Aitken|           30193.0|\n",
      "|     Rio| Haryanto|          25671.16|\n",
      "+--------+---------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display first five rows of dataframe\n",
    "avgPitstopTimeByDriver.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9af74c7-7096-4a16-8065-0d48cfdd992d",
   "metadata": {},
   "source": [
    "## Challenge 2: Insert the missing code (e.g: ALO for Alonso) for all drivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6a99cd1-39a8-4651-adbe-688abb2a40fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|code|count|\n",
      "+----+-----+\n",
      "|  \\N|  757|\n",
      "| ZON|    1|\n",
      "| YAM|    1|\n",
      "| WUR|    1|\n",
      "| WIN|    1|\n",
      "| WEH|    1|\n",
      "| WEB|    1|\n",
      "| VIL|    1|\n",
      "| VET|    1|\n",
      "| VER|    2|\n",
      "+----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "drivers_df.groupby('code').count().orderBy('code', ascending = False).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a792690-c8b8-4501-8528-3fc8bc078525",
   "metadata": {},
   "outputs": [],
   "source": [
    "drivers_clean_code_df = prep.populateMissingDriverCodes(drivers_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac31c8f7-3134-48b7-93b3-0b1d25740ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+----+---------+----------+----------+-----------+--------------------+\n",
      "|driverId| driverRef|number|code| forename|   surname|       dob|nationality|                 url|\n",
      "+--------+----------+------+----+---------+----------+----------+-----------+--------------------+\n",
      "|       1|  hamilton|    44| HAM|    Lewis|  Hamilton|1985-01-07|    British|http://en.wikiped...|\n",
      "|       2|  heidfeld|    \\N| HEI|     Nick|  Heidfeld|1977-05-10|     German|http://en.wikiped...|\n",
      "|       3|   rosberg|     6| ROS|     Nico|   Rosberg|1985-06-27|     German|http://en.wikiped...|\n",
      "|       4|    alonso|    14| ALO| Fernando|    Alonso|1981-07-29|    Spanish|http://en.wikiped...|\n",
      "|       5|kovalainen|    \\N| KOV|   Heikki|Kovalainen|1981-10-19|    Finnish|http://en.wikiped...|\n",
      "|       6|  nakajima|    \\N| NAK|   Kazuki|  Nakajima|1985-01-11|   Japanese|http://en.wikiped...|\n",
      "|       7|  bourdais|    \\N| BOU|S√©bastien|  Bourdais|1979-02-28|     French|http://en.wikiped...|\n",
      "|       8| raikkonen|     7| RAI|     Kimi| R√§ikk√∂nen|1979-10-17|    Finnish|http://en.wikiped...|\n",
      "|       9|    kubica|    88| KUB|   Robert|    Kubica|1984-12-07|     Polish|http://en.wikiped...|\n",
      "|      10|     glock|    \\N| GLO|     Timo|     Glock|1982-03-18|     German|http://en.wikiped...|\n",
      "+--------+----------+------+----+---------+----------+----------+-----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "drivers_clean_code_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3eeb91f4-3eff-4251-81e2-43c34c870b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+------+----+--------+-------+---+-----------+---+\n",
      "|driverId|driverRef|number|code|forename|surname|dob|nationality|url|\n",
      "+--------+---------+------+----+--------+-------+---+-----------+---+\n",
      "+--------+---------+------+----+--------+-------+---+-----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "drivers_clean_code_df.filter(drivers_clean_code_df.code == '\\\\N').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3525de0-00b3-4b9d-abd0-07986361ee71",
   "metadata": {},
   "source": [
    "## Challenge 3: Select a season from the data and determine who was the youngest and oldest at the start of the season and the end of the season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bc72dc3-7d13-4159-ad89-786c21658f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----+---------+--------------------+----------+--------+--------------------+\n",
      "|raceId|year|round|circuitId|                name|      date|    time|                 url|\n",
      "+------+----+-----+---------+--------------------+----------+--------+--------------------+\n",
      "|     1|2009|    1|        1|Australian Grand ...|2009-03-29|06:00:00|http://en.wikiped...|\n",
      "|     2|2009|    2|        2|Malaysian Grand Prix|2009-04-05|09:00:00|http://en.wikiped...|\n",
      "+------+----+-----+---------+--------------------+----------+--------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "races_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "822972ed-f634-4912-bd9b-aed19b6fc295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+--------+-------------+------+----+--------+------------+-------------+------+----+-----------+------------+----------+----+--------------+---------------+--------+\n",
      "|resultId|raceId|driverId|constructorId|number|grid|position|positionText|positionOrder|points|laps|       time|milliseconds|fastestLap|rank|fastestLapTime|fastestLapSpeed|statusId|\n",
      "+--------+------+--------+-------------+------+----+--------+------------+-------------+------+----+-----------+------------+----------+----+--------------+---------------+--------+\n",
      "|       1|    18|       1|            1|    22|   1|       1|           1|            1|  10.0|  58|1:34:50.616|     5690616|        39|   2|      1:27.452|        218.300|       1|\n",
      "|       2|    18|       2|            2|     3|   5|       2|           2|            2|   8.0|  58|     +5.478|     5696094|        41|   3|      1:27.739|        217.586|       1|\n",
      "+--------+------+--------+-------------+------+----+--------+------------+-------------+------+----+-----------+------------+----------+----+--------------+---------------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50c09d55-4d3c-448e-96a6-adc9bb733d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+------+----+--------+--------+----------+-----------+--------------------+\n",
      "|driverId|driverRef|number|code|forename| surname|       dob|nationality|                 url|\n",
      "+--------+---------+------+----+--------+--------+----------+-----------+--------------------+\n",
      "|       1| hamilton|    44| HAM|   Lewis|Hamilton|1985-01-07|    British|http://en.wikiped...|\n",
      "|       2| heidfeld|    \\N| HEI|    Nick|Heidfeld|1977-05-10|     German|http://en.wikiped...|\n",
      "+--------+---------+------+----+--------+--------+----------+-----------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "drivers_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ebc6eca-2718-4253-990c-d74900dc8e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      date|\n",
      "+----------+\n",
      "|2009-03-29|\n",
      "|2009-04-05|\n",
      "|2009-04-19|\n",
      "|2009-04-26|\n",
      "|2009-05-10|\n",
      "|2009-05-24|\n",
      "|2009-06-07|\n",
      "|2009-06-21|\n",
      "|2009-07-12|\n",
      "|2009-07-26|\n",
      "|2009-08-23|\n",
      "|2009-08-30|\n",
      "|2009-09-13|\n",
      "|2009-09-27|\n",
      "|2009-10-04|\n",
      "|2009-10-18|\n",
      "|2009-11-01|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Identify all distinct dates for a year  (2009)\n",
    "races_df.select('date').filter(races_df.year == 2009).distinct().orderBy('date').show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7b4fd56-91f8-4c32-8915-7703eaa3893e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variable for year for season, start date of the season, and end date of the season\n",
    "year = 2009\n",
    "startDate = '2009-03-29'\n",
    "endDate = '2009-11-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8cd16ad7-66ae-4597-99c1-9476cdd9f723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe to hold data for challenge 3\n",
    "driversAgeForSeason = prep.findAgePerSeason(drivers_df, results_df, races_df, startDate, endDate, year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1854b706-97fa-46dd-9437-1c712604c8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+----------+----+----------+---+\n",
      "| forename|    surname|       dob|year|      date|age|\n",
      "+---------+-----------+----------+----+----------+---+\n",
      "|S√©bastien|      Buemi|1988-10-31|2009|2009-03-29| 20|\n",
      "|   Rubens|Barrichello|1972-05-23|2009|2009-03-29| 36|\n",
      "|    Jaime|Alguersuari|1990-03-23|2009|2009-11-01| 19|\n",
      "|   Rubens|Barrichello|1972-05-23|2009|2009-11-01| 37|\n",
      "+---------+-----------+----------+----+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "driversAgeForSeason.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0b8a4b-70ac-484c-9db7-7e629f86f6a9",
   "metadata": {},
   "source": [
    "## Challenge 4: Which driver has the most wins and which driver has the most losses for each Grand Prix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5595815-a0f1-4c10-9614-524a45bdb2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mostWinsByGrandPrix = prep.mostWinByGrandPrix(races_df, results_df, drivers_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e647a797-ebdf-4798-bab6-6b174e679b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+----------+-----+----------+\n",
      "|                name|forename|   surname|count|rankByWins|\n",
      "+--------------------+--------+----------+-----+----------+\n",
      "|   Sakhir Grand Prix|  Sergio|     P√©rez|    1|         1|\n",
      "|  Pescara Grand Prix|Stirling|      Moss|    1|         1|\n",
      "|Luxembourg Grand ...| Jacques|Villeneuve|    1|         1|\n",
      "|Luxembourg Grand ...|    Mika|  H√§kkinen|    1|         1|\n",
      "|    Indianapolis 500|    Bill|  Vukovich|    2|         1|\n",
      "|United States Gra...|  Nelson|    Piquet|    1|         1|\n",
      "|United States Gra...|  Carlos| Reutemann|    1|         1|\n",
      "|United States Gra...|    John|    Watson|    1|         1|\n",
      "|United States Gra...|   Mario|  Andretti|    1|         1|\n",
      "|United States Gra...|    Clay| Regazzoni|    1|         1|\n",
      "+--------------------+--------+----------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mostWinsByGrandPrix.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ea31641-94e7-4acd-9d3d-dc63d5ffa7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mostLossByGrandPrix = prep.mostLossByGrandPrix(races_df, results_df, drivers_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c788398-c86c-4d74-9eeb-2197d9983c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------+----------+-----+----------+\n",
      "|             name| forename|   surname|count|rankByWins|\n",
      "+-----------------+---------+----------+-----+----------+\n",
      "|Sakhir Grand Prix|  Antonio|Giovinazzi|    1|         1|\n",
      "|Sakhir Grand Prix|    Kevin| Magnussen|    1|         1|\n",
      "|Sakhir Grand Prix|    Lance|    Stroll|    1|         1|\n",
      "|Sakhir Grand Prix|   Daniel| Ricciardo|    1|         1|\n",
      "|Sakhir Grand Prix|     Kimi| R√§ikk√∂nen|    1|         1|\n",
      "|Sakhir Grand Prix|Alexander|     Albon|    1|         1|\n",
      "|Sakhir Grand Prix|Sebastian|    Vettel|    1|         1|\n",
      "|Sakhir Grand Prix|   Daniil|     Kvyat|    1|         1|\n",
      "|Sakhir Grand Prix|    Lando|    Norris|    1|         1|\n",
      "|Sakhir Grand Prix|     Jack|    Aitken|    1|         1|\n",
      "+-----------------+---------+----------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mostLossByGrandPrix.show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
